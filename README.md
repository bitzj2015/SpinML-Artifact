# SpinML-Artifact

Paper title: **SpinML: Customized Synthetic Data Generation for Private Training of Specialized ML Models**

Artifacts HotCRP Id: **#18** (not your paper Id, but the artifacts id)

Requested Badge: **Available**, **Functional**

## Description
This artifact contains the scripts for running experiments described in this paper.

### Security/Privacy Issues and Ethical Concerns (All badges)
Our artifact does not hold any risk to the security or privacy of the reviewer's machine. Also, our artifact does not contain malware samples, or something similar, to be analyzed. In addition, there are not any ethical concerns regarding our artifacts here.

## Basic Requirements (Only for Functional and Reproduced badges)
The artifact requires the following minimal hardware and software configurations.

### Hardware Requirements
We conducted experiments on a server equipped with:

CPU: `AMD Threadripper 3970X`
GPU: Two `NVIDIA RTX A6000` (each with 48GB memory)
Memory: 128GB RAM
While these are not strict requirements, we recommend using a server with at least 16GB of GPU memory to successfully reproduce the experiments.

### Software Requirements
The experiments were performed on a server running `Ubuntu 20.04.1 LTS`. The following Python libraries are required to run the artifact:
```
pandas==1.5.2
pillow==10.2.0
torch==2.1.0
torchvision==0.16.0
tqdm==4.65.2
transformers==4.46.3
controlnet-aux==0.0.7
```


### Estimated Time and Storage Consumption
Provide an estimated value for the time the evaluation will take and the space on the disk it will consume. 
This helps reviewers to schedule the evaluation in their time plan and to see if everything is running as intended.
More specifically, a reviewer, who knows that the evaluation might take 10 hours, does not expect an error if, after 1 hour, the computer is still calculating things.

## Environment 
In the following, describe how to access our artifact and all related and necessary data and software components.
Afterward, describe how to set up everything and how to verify that everything is set up correctly.

### Accessibility (All badges)
Describe how to access your artifact via persistent sources.
Valid hosting options are institutional and third-party digital repositories.
Do not use personal web pages.
For repositories that evolve over time (e.g., Git Repositories ), specify a specific commit-id or tag to be evaluated.
In case your repository changes during the evaluation to address the reviewer's feedback, please provide an updated link (or commit-id / tag) in a comment.

### Set up the environment (Only for Functional and Reproduced badges)
Describe how the reviewers should set up the environment for your artifact, including downloading and installing dependencies and the installation of the artifact itself.
Be as specific as possible here.
If possible, use code segments to simply the workflow, e.g.,

```bash
git clone git@github.com:bitzj2015/SpinML-Artifact.git
```
Describe the expected results where it makes sense to do so.

### Testing the Environment (Only for Functional and Reproduced badges)
Describe the basic functionality tests to check if the environment is set up correctly.
These tests could be unit tests, training an ML model on very low training data, etc..
If these tests succeed, all required software should be functioning correctly.
Include the expected output for unambiguous outputs of tests.
Use code segments to simplify the workflow, e.g.,
```bash
python envtest.py
```

## Artifact Evaluation (Only for Functional and Reproduced badges)
This section includes all the steps required to evaluate your artifact's functionality and validate your paper's key results and claims.
Therefore, highlight your paper's main results and claims in the first subsection. And describe the experiments that support your claims in the subsection after that.

### Main Results and Claims
List all your paper's results and claims that are supported by your submitted artifacts.

#### Main Result 1: Name
Describe the results in 1 to 3 sentences.
Refer to the related sections in your paper and reference the experiments that support this result/claim.

#### Main Result 2: Name
...

### Experiments 
List each experiment the reviewer has to execute. Describe:
 - How to execute it in detailed steps.
 - What the expected result is.
 - How long it takes and how much space it consumes on disk. (approximately)
 - Which claim and results does it support, and how.

#### Experiment 1: Name
Provide a short explanation of the experiment and expected results.
Describe thoroughly the steps to perform the experiment and to collect and organize the results as expected from your paper.
Use code segments to support the reviewers, e.g.,
```bash
python experiment_1.py
```
#### Experiment 2: Name
...

#### Experiment 3: Name 
...

## Limitations (Only for Functional and Reproduced badges)
Describe which tables and results are included or are not reproducible with the provided artifact.
Provide an argument why this is not included/possible.

## Notes on Reusability (Only for Functional and Reproduced badges)
First, this section might not apply to your artifacts.
Use it to share information on how your artifact can be used beyond your research paper, e.g., as a general framework.
The overall goal of artifact evaluation is not only to reproduce and verify your research but also to help other researchers to re-use and improve on your artifacts.
Please describe how your artifacts can be adapted to other settings, e.g., more input dimensions, other datasets, and other behavior, through replacing individual modules and functionality or running more iterations of a specific part.